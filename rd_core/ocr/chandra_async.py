"""Async Chandra OCR Backend (LM Studio / OpenAI-compatible API)"""
import logging
import os
from typing import Optional

import httpx
from PIL import Image

from rd_core.ocr.chandra import (
    CHANDRA_DEFAULT_PROMPT,
    CHANDRA_DEFAULT_SYSTEM,
    CHANDRA_LOAD_CONFIG,
    CHANDRA_MODEL_KEY,
)
from rd_core.ocr.utils import image_to_base64

logger = logging.getLogger(__name__)


class AsyncChandraBackend:
    """Асинхронный OCR через Chandra модель (LM Studio, OpenAI-compatible API)"""

    DEFAULT_BASE_URL = "https://louvred-madie-gigglier.ngrok-free.dev"

    def __init__(self, base_url: Optional[str] = None):
        self.base_url = base_url or os.getenv("CHANDRA_BASE_URL", self.DEFAULT_BASE_URL)
        self._model_id: Optional[str] = None
        self._client: Optional[httpx.AsyncClient] = None

        logger.info(f"AsyncChandraBackend инициализирован (base_url: {self.base_url})")

    async def _get_client(self) -> httpx.AsyncClient:
        """Получить или создать httpx AsyncClient с connection pooling"""
        if self._client is None or self._client.is_closed:
            transport = httpx.AsyncHTTPTransport(
                retries=3,
                limits=httpx.Limits(
                    max_connections=10,
                    max_keepalive_connections=5,
                    keepalive_expiry=30.0,
                ),
            )
            self._client = httpx.AsyncClient(
                transport=transport,
                timeout=httpx.Timeout(300.0, connect=30.0),
            )
        return self._client

    async def close(self):
        """Закрыть HTTP клиент"""
        if self._client and not self._client.is_closed:
            await self._client.aclose()
            self._client = None

    async def _discover_model(self) -> str:
        """Авто-определение модели через /v1/models + preload через native API"""
        if self._model_id:
            return self._model_id

        await self._ensure_model_loaded()

        try:
            client = await self._get_client()
            resp = await client.get(
                f"{self.base_url}/v1/models",
                timeout=30.0,
            )
            if resp.status_code == 200:
                for m in resp.json().get("data", []):
                    if "chandra" in m.get("id", "").lower():
                        self._model_id = m["id"]
                        logger.info(f"Chandra модель найдена: {self._model_id}")
                        return self._model_id
        except Exception as e:
            logger.warning(f"Ошибка определения модели Chandra: {e}")

        self._model_id = "chandra-ocr"
        logger.info(f"Chandra модель не найдена, используется fallback: {self._model_id}")
        return self._model_id

    @staticmethod
    def _needs_reload(loaded_instances: list, required_context: int) -> tuple:
        """Проверяет нужна ли перезагрузка модели из-за несовпадения context_length."""
        if not loaded_instances:
            return True, "модель не загружена"
        for inst in loaded_instances:
            inst_id = inst.get("id", "unknown")
            ctx = inst.get("context_length")
            if ctx is None:
                return True, f"instance {inst_id}: context_length недоступен в API"
            if ctx != required_context:
                return True, f"instance {inst_id}: context_length={ctx}, требуется {required_context}"
        return False, f"context_length={required_context} OK"

    async def _ensure_model_loaded(self) -> None:
        """
        Проверяет загружена ли модель через LM Studio native API.
        Если нет или context_length не совпадает — выгружает и загружает с правильным конфигом.
        """
        required_ctx = CHANDRA_LOAD_CONFIG["context_length"]
        try:
            client = await self._get_client()
            resp = await client.get(
                f"{self.base_url}/api/v1/models",
                timeout=10.0,
            )
            if resp.status_code != 200:
                logger.debug("LM Studio native API недоступен, пропускаем preload")
                return

            models = resp.json().get("models", [])

            for m in models:
                if "chandra" in m.get("key", "").lower():
                    loaded = m.get("loaded_instances", [])
                    needs_reload, reason = self._needs_reload(loaded, required_ctx)

                    if not needs_reload:
                        logger.debug(f"Модель {m['key']}: {reason}")
                        return

                    logger.info(f"Модель {m['key']}: {reason}, выполняем reload")
                    for inst in loaded:
                        try:
                            await client.post(
                                f"{self.base_url}/api/v1/models/unload",
                                json={"instance_id": inst["id"]},
                                timeout=30.0,
                            )
                            logger.debug(f"Выгружен инстанс: {inst['id']}")
                        except Exception as e:
                            logger.warning(f"Ошибка выгрузки {inst.get('id')}: {e}")
                    break

            logger.info(
                f"Загружаем модель {CHANDRA_MODEL_KEY} "
                f"(context_length={required_ctx})"
            )
            load_resp = await client.post(
                f"{self.base_url}/api/v1/models/load",
                json={"model": CHANDRA_MODEL_KEY, "echo_load_config": True, **CHANDRA_LOAD_CONFIG},
                timeout=120.0,
            )

            if load_resp.status_code == 200:
                load_data = load_resp.json()
                actual_ctx = load_data.get("load_config", {}).get("context_length", "?")
                logger.info(
                    f"Модель загружена: context_length={actual_ctx}, "
                    f"время={load_data.get('load_time_seconds', '?')}с"
                )
            else:
                logger.warning(
                    f"Ошибка загрузки: {load_resp.status_code} - {load_resp.text[:300]}"
                )

        except Exception as e:
            logger.debug(f"Native API preload недоступен: {e}")

    def unload_model(self) -> None:
        """Выгрузить модель из LM Studio (освобождает VRAM). Sync для finally."""
        import httpx as _httpx

        if not self._model_id:
            return
        try:
            resp = _httpx.get(
                f"{self.base_url}/api/v1/models",
                timeout=10.0,
            )
            if resp.status_code != 200:
                return

            models = resp.json().get("models", [])
            for m in models:
                if "chandra" in m.get("key", "").lower():
                    for inst in m.get("loaded_instances", []):
                        _httpx.post(
                            f"{self.base_url}/api/v1/models/unload",
                            json={"instance_id": inst["id"]},
                            timeout=30.0,
                        )
                        logger.info(f"Модель выгружена: {inst['id']}")
                    break
        except Exception as e:
            logger.debug(f"Ошибка выгрузки модели: {e}")

    def supports_pdf_input(self) -> bool:
        """Chandra не поддерживает прямой ввод PDF"""
        return False

    async def recognize_async(
        self,
        image: Optional[Image.Image],
        prompt: Optional[dict] = None,
        json_mode: bool = None,
        pdf_file_path: Optional[str] = None,
    ) -> str:
        """Асинхронно распознать текст через Chandra (LM Studio API)"""
        if image is None:
            return "[Ошибка: Chandra требует изображение]"

        try:
            client = await self._get_client()
            model_id = await self._discover_model()
            img_b64 = image_to_base64(image)

            # Chandra всегда использует свой специализированный HTML промпт
            # System prompt берём из переданного dict (контекст задачи)
            if prompt and isinstance(prompt, dict):
                system_prompt = prompt.get("system", "") or CHANDRA_DEFAULT_SYSTEM
            else:
                system_prompt = CHANDRA_DEFAULT_SYSTEM
            user_prompt = CHANDRA_DEFAULT_PROMPT

            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append(
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{img_b64}"
                            },
                        },
                        {
                            "type": "text",
                            "text": user_prompt,
                        },
                    ],
                }
            )

            payload = {
                "model": model_id,
                "messages": messages,
                "max_tokens": 12384,
                "temperature": 0,
                "top_p": 0.1,
            }

            response = await client.post(
                f"{self.base_url}/v1/chat/completions",
                headers={"Content-Type": "application/json"},
                json=payload,
            )

            if response.status_code != 200:
                error_detail = response.text[:500] if response.text else "No details"
                logger.error(
                    f"Chandra API error: {response.status_code} - {error_detail}"
                )
                return f"[Ошибка Chandra API: {response.status_code}]"

            result = response.json()
            text = result["choices"][0]["message"]["content"].strip()
            logger.debug(f"AsyncChandra OCR: распознано {len(text)} символов")
            return text

        except httpx.TimeoutException:
            logger.error("AsyncChandra OCR: превышен таймаут")
            return "[Ошибка: превышен таймаут запроса к Chandra]"
        except Exception as e:
            logger.error(f"Ошибка AsyncChandra OCR: {e}", exc_info=True)
            return f"[Ошибка Chandra OCR: {e}]"

    def __del__(self):
        """Cleanup при удалении объекта"""
        if self._client and not self._client.is_closed:
            logger.debug("AsyncChandraBackend: client not closed properly")
