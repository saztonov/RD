# Локальный VLM сервер для OCR

Приложение поддерживает использование локальных VLM (Vision Language Model) серверов для OCR.

## Поддерживаемые модели

- **Qwen3-VL** (Qwen3-VL-32B-Instruct и др.)
- **LLaVA**
- Любые VLM модели с OpenAI-совместимым API

## Настройка

### 1. Запустите VLM сервер

Используйте любой сервер с OpenAI-совместимым API, например:

**LM Studio:**
```bash
# Загрузите модель qwen3-vl-32b-instruct
# Запустите сервер на http://127.0.0.1:1234
```

**vLLM:**
```bash
vllm serve qwen/Qwen3-VL-32B-Instruct \
  --api-key token-abc123 \
  --port 1234
```

**Ollama:**
```bash
ollama run qwen3-vl:32b
# Сервер на http://127.0.0.1:11434
```

### 2. Используйте в приложении

1. Откройте PDF: **Файл → Открыть PDF**
2. Запустите OCR: **Инструменты → Запустить OCR** (Ctrl+R)
3. Выберите:
   - **Локальный VLM сервер**
   - URL сервера: `http://127.0.0.1:1234/v1`
   - Модель: `qwen3-vl-32b-instruct`
4. Режим:
   - **По блокам**: распознает ваши размеченные блоки
   - **Вся страница**: полная автоматическая структура

## Конфигурация по умолчанию

- **URL сервера**: `http://127.0.0.1:1234/v1`
- **Модель**: `qwen3-vl-32b-instruct`
- **Таймаут**: 120 секунд
- **Max tokens**: 4096

## Использование через код

```python
from app.ocr import LocalVLMBackend
from PIL import Image

# Создать движок
ocr = LocalVLMBackend(
    api_base="http://127.0.0.1:1234/v1",
    model_name="qwen3-vl-32b-instruct"
)

# Распознать изображение
image = Image.open("document.png")
text = ocr.recognize(image)
print(text)
```

## Промпт для OCR

По умолчанию используется промпт:
```
Распознай весь текст на этом изображении.
Сохрани структуру документа, таблицы представь в Markdown формате.
Выведи только распознанный текст без комментариев.
```

Вы можете передать кастомный промпт через параметр `prompt` в методе `recognize()`.

## Требования

- Запущенный VLM сервер с OpenAI API
- Библиотека `requests` (установлена автоматически)
- Достаточная VRAM для модели (например, 32GB для Qwen3-VL-32B)

## Преимущества локального VLM

✅ **Полный контроль** над моделью и данными  
✅ **Приватность** - данные не покидают вашу машину  
✅ **Кастомизация** промптов под ваши задачи  
✅ **Без лимитов** API запросов  
✅ **Высокая точность** на русском языке (Qwen3-VL)

## Сравнение с Chandra OCR

| Параметр | LocalVLM | Chandra OCR |
|----------|----------|-------------|
| Установка | Требует VLM сервер | pip install chandra-ocr |
| Скорость | Зависит от GPU | Оптимизирована |
| Точность | Зависит от модели | Высокая на всех типах |
| Гибкость | Полная (промпты) | Ограниченная |
| VRAM | 16-64GB | 8-16GB |

## Рекомендации

- Для **общих документов**: Chandra OCR
- Для **русского языка**: LocalVLM (Qwen3-VL)
- Для **кастомных задач**: LocalVLM с промптами
- Для **быстрой обработки**: vLLM сервер

## Troubleshooting

**Ошибка подключения к серверу:**
- Проверьте, что сервер запущен
- Проверьте URL (должен заканчиваться на `/v1`)
- Проверьте firewall/антивирус

**Пустой ответ от сервера:**
- Проверьте имя модели
- Увеличьте `max_tokens`
- Проверьте логи сервера

**Таймаут:**
- Увеличьте таймаут в коде (параметр `timeout`)
- Проверьте загрузку GPU
- Используйте меньшую модель

