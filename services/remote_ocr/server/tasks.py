"""Celery –∑–∞–¥–∞—á–∏ –¥–ª—è OCR –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
from __future__ import annotations

import json
import shutil
import tempfile
import traceback
from datetime import datetime
from pathlib import Path

from .celery_app import celery_app
from .db_metrics import get_metrics_collector
from .debounced_updater import cleanup_updater, get_debounced_updater
from .logging_config import get_logger
from .memory_utils import force_gc, log_memory, log_memory_delta
from .rate_limiter import get_datalab_limiter
from .settings import settings
from .storage import get_job, register_ocr_results_to_node, update_job_status
from .storage_jobs import increment_retry_count, set_job_started_at
from .task_helpers import check_paused, create_empty_result, download_job_files
from .task_ocr_twopass import run_two_pass_ocr
from .task_results import generate_results
from .task_upload import upload_results_to_r2
from .worker_pdf import clear_page_size_cache

logger = get_logger(__name__)


@celery_app.task(bind=True, name="run_ocr_task", max_retries=3, rate_limit="4/m")
def run_ocr_task(self, job_id: str) -> dict:
    """Celery –∑–∞–¥–∞—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ OCR"""
    start_mem = log_memory(f"[START] –ó–∞–¥–∞—á–∞ {job_id}")

    work_dir = None
    try:
        # –ü–æ–ª—É—á–∞–µ–º –∑–∞–¥–∞—á—É –∏–∑ –ë–î —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        job = get_job(job_id, with_files=True, with_settings=True)
        if not job:
            logger.error(f"–ó–∞–¥–∞—á–∞ {job_id} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return {"status": "error", "message": "Job not found"}

        # ===== –ó–ê–©–ò–¢–ê –û–¢ –ó–ê–¶–ò–ö–õ–ò–í–ê–ù–ò–Ø =====
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫
        if job.retry_count >= settings.job_max_retries:
            error_msg = f"–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –ø–æ–ø—ã—Ç–æ–∫: {job.retry_count}/{settings.job_max_retries}"
            logger.error(f"Job {job_id}: {error_msg}")
            update_job_status(
                job_id, "error",
                error_message=error_msg,
                status_message="‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –ø–æ–ø—ã—Ç–æ–∫"
            )
            return {"status": "error", "message": "Max retries exceeded"}

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –æ–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        if job.started_at:
            try:
                # –ü–∞—Ä—Å–∏–º ISO —Ñ–æ—Ä–º–∞—Ç —Å —É—á—ë—Ç–æ–º –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ –Ω–∞–ª–∏—á–∏—è 'Z' –∏–ª–∏ '+00:00'
                started_str = job.started_at.replace('Z', '+00:00')
                if '+' not in started_str and started_str.endswith('+00:00') is False:
                    started = datetime.fromisoformat(started_str)
                else:
                    # –£–±–∏—Ä–∞–µ–º timezone info –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å utcnow()
                    started = datetime.fromisoformat(started_str.split('+')[0])
                runtime_hours = (datetime.utcnow() - started).total_seconds() / 3600

                if runtime_hours > settings.job_max_runtime_hours:
                    error_msg = f"–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {runtime_hours:.1f}h (–ª–∏–º–∏—Ç: {settings.job_max_runtime_hours}h)"
                    logger.error(f"Job {job_id}: {error_msg}")
                    update_job_status(
                        job_id, "error",
                        error_message=error_msg,
                        status_message="‚ùå –ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è"
                    )
                    return {"status": "error", "message": "Max runtime exceeded"}
            except Exception as e:
                logger.warning(f"Job {job_id}: –æ—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ started_at ({job.started_at}): {e}")

        # –ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∏—Ä—É–µ–º retry_count
        new_retry_count = increment_retry_count(job_id)
        logger.info(f"Job {job_id}: –ø–æ–ø—ã—Ç–∫–∞ {new_retry_count}/{settings.job_max_retries}")

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º started_at —Ç–æ–ª—å–∫–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ
        if not job.started_at:
            set_job_started_at(job_id)
        # ===== –ö–û–ù–ï–¶ –ó–ê–©–ò–¢–´ –û–¢ –ó–ê–¶–ò–ö–õ–ò–í–ê–ù–ò–Ø =====

        if check_paused(job.id):
            return {"status": "paused"}

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ processing
        update_job_status(job.id, "processing", progress=0.05, status_message="üì• –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–¥–∞—á–∏...")

        # –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        work_dir = Path(tempfile.mkdtemp(prefix=f"ocr_job_{job.id}_"))
        crops_dir = work_dir / "crops"
        crops_dir.mkdir(exist_ok=True)

        logger.info(f"–ó–∞–¥–∞—á–∞ {job.id}: —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –∏–∑ R2...")
        update_job_status(job.id, "processing", progress=0.06, status_message="üì• –°–∫–∞—á–∏–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤ –∏–∑ R2...")
        pdf_path, blocks_path = download_job_files(job, work_dir)
        log_memory_delta("–ü–æ—Å–ª–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Ñ–∞–π–ª–æ–≤", start_mem)

        with open(blocks_path, "r", encoding="utf-8") as f:
            blocks_data = json.load(f)

        # annotation.json –∏–º–µ–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É {pdf_path, pages: [{blocks: [...]}]}
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –±–ª–æ–∫–∏ –∏–∑ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        if isinstance(blocks_data, dict) and "pages" in blocks_data:
            all_blocks = []
            for page in blocks_data.get("pages", []):
                all_blocks.extend(page.get("blocks", []))
            blocks_data = all_blocks

        if not blocks_data:
            update_job_status(job.id, "done", progress=1.0, status_message="‚úÖ –ù–µ—Ç –±–ª–æ–∫–æ–≤ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è")
            create_empty_result(job, work_dir, pdf_path)
            upload_results_to_r2(job, work_dir)
            return {"status": "done", "job_id": job_id}

        from rd_core.models import Block
        from rd_core.ocr import create_ocr_engine

        blocks = [Block.from_dict(b, migrate_ids=False)[0] for b in blocks_data]
        total_blocks = len(blocks)

        logger.info(f"–ó–∞–¥–∞—á–∞ {job.id}: {total_blocks} –±–ª–æ–∫–æ–≤")

        if check_paused(job.id):
            return {"status": "paused"}

        update_job_status(job.id, "processing", progress=0.1, status_message=f"‚öôÔ∏è –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞: {total_blocks} –±–ª–æ–∫–æ–≤")

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ Supabase
        job_settings = job.settings
        text_model = (job_settings.text_model if job_settings else "") or ""
        table_model = (job_settings.table_model if job_settings else "") or ""
        image_model = (job_settings.image_model if job_settings else "") or ""
        stamp_model = (job_settings.stamp_model if job_settings else "") or ""

        engine = job.engine or "openrouter"
        datalab_limiter = get_datalab_limiter() if engine == "datalab" else None

        if engine == "chandra" and settings.chandra_base_url:
            strip_backend = create_ocr_engine(
                "chandra",
                base_url=settings.chandra_base_url,
            )
        elif engine == "datalab" and settings.datalab_api_key:
            strip_backend = create_ocr_engine(
                "datalab",
                api_key=settings.datalab_api_key,
                rate_limiter=datalab_limiter,
                poll_interval=settings.datalab_poll_interval,
                poll_max_attempts=settings.datalab_poll_max_attempts,
                max_retries=settings.datalab_max_retries,
            )
        elif settings.openrouter_api_key:
            strip_model = text_model or table_model or "qwen/qwen3-vl-30b-a3b-instruct"
            strip_backend = create_ocr_engine(
                "openrouter",
                api_key=settings.openrouter_api_key,
                model_name=strip_model,
                base_url=settings.openrouter_base_url,
            )
        else:
            strip_backend = create_ocr_engine("dummy")

        if settings.openrouter_api_key:
            img_model = (
                image_model
                or text_model
                or table_model
                or "qwen/qwen3-vl-30b-a3b-instruct"
            )
            logger.info(f"IMAGE –º–æ–¥–µ–ª—å: {img_model}")
            image_backend = create_ocr_engine(
                "openrouter",
                api_key=settings.openrouter_api_key,
                model_name=img_model,
                base_url=settings.openrouter_base_url,
            )

            stmp_model = (
                stamp_model
                or image_model
                or text_model
                or table_model
                or "qwen/qwen3-vl-30b-a3b-instruct"
            )
            logger.info(f"STAMP –º–æ–¥–µ–ª—å: {stmp_model}")
            stamp_backend = create_ocr_engine(
                "openrouter",
                api_key=settings.openrouter_api_key,
                model_name=stmp_model,
                base_url=settings.openrouter_base_url,
            )
        else:
            image_backend = create_ocr_engine("dummy")
            stamp_backend = create_ocr_engine("dummy")

        # OCR –æ–±—Ä–∞–±–æ—Ç–∫–∞ (–¥–≤—É—Ö–ø—Ä–æ—Ö–æ–¥–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º)
        run_two_pass_ocr(
            job,
            pdf_path,
            blocks,
            crops_dir,
            work_dir,
            strip_backend,
            image_backend,
            stamp_backend,
            start_mem,
            engine=engine,
        )

        force_gc("–ø–æ—Å–ª–µ OCR –æ–±—Ä–∞–±–æ—Ç–∫–∏")

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–ø–µ—Ä–µ–¥–∞—ë–º datalab backend –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏)
        update_job_status(job.id, "processing", progress=0.92, status_message="üìÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        verification_backend = strip_backend if engine == "datalab" else None

        # Callback –¥–ª—è –≤–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏–∏ –±–ª–æ–∫–æ–≤ (–¥–∏–∞–ø–∞–∑–æ–Ω 0.92 -> 0.94)
        def on_verification_progress(current: int, total: int):
            if total > 0:
                progress = 0.92 + 0.02 * (current / total)
                status_msg = f"üîç –í–µ—Ä–∏—Ñ–∏–∫–∞—Ü–∏—è –±–ª–æ–∫–æ–≤ ({current + 1}/{total})"
            else:
                progress = 0.92
                status_msg = "üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤..."

            # –§–æ—Ä—Å–∏—Ä—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è –≤–∞–∂–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤ (–Ω–∞—á–∞–ª–æ, –∫–∞–∂–¥—ã–π 5-–π –±–ª–æ–∫, –∫–æ–Ω–µ—Ü)
            updater = get_debounced_updater(job.id)
            if total == 0 or current == 0 or current == total - 1 or current % 5 == 0:
                updater.force_update("processing", progress=progress, status_message=status_msg)
            else:
                update_job_status(job.id, "processing", progress=progress, status_message=status_msg)

        r2_prefix = generate_results(
            job, pdf_path, blocks, work_dir, verification_backend, on_verification_progress
        )

        # –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ R2
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ R2...")
        update_job_status(job.id, "processing", progress=0.95, status_message="‚òÅÔ∏è –ó–∞–≥—Ä—É–∑–∫–∞ –≤ –æ–±–ª–∞–∫–æ...")
        upload_results_to_r2(job, work_dir, r2_prefix)

        # –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è OCR —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ node_files
        if job.node_id:
            update_job_status(job.id, "processing", progress=0.98, status_message="üìù –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —Ñ–∞–π–ª–æ–≤...")
            registered_count = register_ocr_results_to_node(job.node_id, job.document_name, work_dir)
            logger.info(f"‚úÖ –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ {registered_count} —Ñ–∞–π–ª–æ–≤ –≤ node_files –¥–ª—è node {job.node_id}")

            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å PDF –¥–æ–∫—É–º–µ–Ω—Ç–∞
            try:
                from .node_storage import update_node_pdf_status

                update_node_pdf_status(job.node_id)
                logger.info(f"PDF status updated for node {job.node_id}")
            except Exception as e:
                logger.warning(f"Failed to update PDF status: {e}")

        update_job_status(job.id, "done", progress=1.0, status_message="‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ")
        logger.info(f"–ó–∞–¥–∞—á–∞ {job.id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ")

        return {"status": "done", "job_id": job_id}

    except Exception as e:
        error_msg = f"{e}\n{traceback.format_exc()}"
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á–∏ {job_id}: {error_msg}")
        update_job_status(job_id, "error", error_message=str(e), status_message="‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return {"status": "error", "message": str(e)}

    finally:
        # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ debounced updater
        stats = cleanup_updater(job_id)
        if stats:
            logger.info(
                f"[METRICS] Job {job_id} status updates: "
                f"{stats['db_calls']} DB calls, {stats['skipped']} skipped "
                f"({stats['reduction_percent']}% reduction)"
            )

        # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ DB
        get_metrics_collector().log_summary(job_id)
        get_metrics_collector().pop_metrics(job_id)

        # –û—á–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        if work_dir and work_dir.exists():
            try:
                shutil.rmtree(work_dir)
                logger.info(f"‚úÖ –í—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞: {work_dir}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {e}")

        # –í—ã–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å Chandra –∏–∑ LM Studio (–æ—Å–≤–æ–±–æ–∂–¥–∞–µ–º VRAM)
        if engine == "chandra" and hasattr(strip_backend, "unload_model"):
            strip_backend.unload_model()

        # –û—á–∏—â–∞–µ–º –∫—ç—à —Ä–∞–∑–º–µ—Ä–æ–≤ —Å—Ç—Ä–∞–Ω–∏—Ü
        clear_page_size_cache()

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
        force_gc("—Ñ–∏–Ω–∞–ª—å–Ω–∞—è")
        log_memory_delta(f"[END] –ó–∞–¥–∞—á–∞ {job_id}", start_mem)
